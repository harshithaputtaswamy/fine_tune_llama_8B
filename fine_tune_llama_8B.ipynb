{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07763454-0155-44a7-8463-014db3df6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.246.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft) (6.1.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: attrs<26,>=24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (25.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.38.35)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.1.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.115.12)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.24.0)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.3.8)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (5.29.5)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.37)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.4.0)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.34.3)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.38.35)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.39.0,>=1.38.35->boto3<2.0,>=1.35.75->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.22.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.35->boto3<2.0,>=1.35.75->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.9.2)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (14.0.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets>=3.0.0->trl)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->sagemaker) (0.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->sagemaker) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2025.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.7)\n",
      "INFO: pip is looking at multiple versions of pathos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker) (0.16.0)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m192.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m164.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m146.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: triton, sentencepiece, nvidia-cusparselt-cu12, xxhash, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface_hub, tokenizers, pathos, nvidia-cusolver-cu12, transformers, torch, datasets, bitsandbytes, accelerate, trl, peft\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/32\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/32\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/32\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: fsspec[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/32\u001b[0m [nvidia-cublas-cu12]u12]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/32\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/32\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/32\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: dill\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [fsspec]as-cu12]\n",
      "\u001b[2K    Found existing installation: dill 0.4.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/32\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling dill-0.4.0:━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/32\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.090m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/32\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: multiprocess[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [nvidia-cudnn-cu12]12]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18━━━━━━━━━\u001b[0m \u001b[32m19/32\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/32\u001b[0m [multiprocess]]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18━━━━━━━━━━━\u001b[0m \u001b[32m20/32\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: pathos━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [tokenizers]_hub]\n",
      "\u001b[2K    Found existing installation: pathos 0.3.4\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling pathos-0.3.4:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [tokenizers]\n",
      "\u001b[2K      Successfully uninstalled pathos-0.3.40m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m22/32\u001b[0m [tokenizers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [peft]2m31/32\u001b[0m [peft]erate]s]er-cu12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 bitsandbytes-0.46.0 datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 hf-xet-1.1.3 huggingface_hub-0.33.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pathos-0.3.2 peft-0.15.2 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.52.4 triton-3.2.0 trl-0.18.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch bitsandbytes peft trl sagemaker huggingface_hub sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4bf18d-5a14-4ab6-91a3-fadde647135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be89c2a9-2d33-48c8-b15c-70dcf7d8ee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::329599621791:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker bucket: sagemaker-us-east-1-329599621791\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f9d21-6ad3-4061-abc6-70a0f575ebf3",
   "metadata": {},
   "source": [
    "Data Loading, Shuffing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dff3231-edb3-4b94-9cfa-0a06b5aa0785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e283e7096b425a8eb181888fc3d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606f9c40f53a4aa5964fa84221476ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890\n"
     ]
    }
   ],
   "source": [
    "def load_shuffle_split_dataset():\n",
    "    raw_data_path = f's3://{sess.default_bucket()}/context_immigration_data.json'\n",
    "    dataset = load_dataset('json', data_files=raw_data_path, split='train')\n",
    "    print(len(dataset))\n",
    "    \n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    train_val_split = dataset.train_test_split(test_size=0.2)\n",
    "    \n",
    "    val_test_split = train_val_split['test'].train_test_split(test_size=0.5)\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_val_split['train'],\n",
    "        'validation': val_test_split['train'],\n",
    "        'test': val_test_split['test']\n",
    "    })\n",
    "    \n",
    "    print(dataset_dict['train'][0])\n",
    "    print(dataset_dict['validation'][0])\n",
    "    print(dataset_dict['test'][0])\n",
    "    \n",
    "    os.makedirs(\"split_data\", exist_ok=True)\n",
    "    \n",
    "    # Save locally first\n",
    "    dataset_dict['train'].to_json(\"split_data/train.jsonl\", lines=True)\n",
    "    dataset_dict['validation'].to_json(\"split_data/validation.jsonl\", lines=True)\n",
    "    dataset_dict['test'].to_json(\"split_data/test.jsonl\", lines=True)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    s3_prefix = 'split_data'\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        local_file = f\"split_data/{split}.jsonl\"\n",
    "        s3_key = f\"{s3_prefix}/{split}.jsonl\"\n",
    "        s3.upload_file(local_file, sess.default_bucket(), s3_key)\n",
    "        print(f\"Uploaded to s3://{sess.default_bucket()}/{s3_key}\")\n",
    "\n",
    "\n",
    "# load_shuffle_split_dataset()\n",
    "train_data_path = f's3://{sess.default_bucket()}/split_data/train.jsonl'\n",
    "train_dataset = load_dataset('json', data_files=train_data_path, split='train', streaming=False)\n",
    "print(len(train_dataset))\n",
    "\n",
    "validation_data_path = f's3://{sess.default_bucket()}/split_data/validation.jsonl'\n",
    "validation_dataset = load_dataset('json', data_files=validation_data_path, split='train', streaming=False)\n",
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06f5b8-8b60-4289-801e-b180e0c1db6d",
   "metadata": {},
   "source": [
    "Tokenazing data, preparing chat tempelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f73aed-deda-46e4-8d30-a9ca53f4df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "def format_chat_template(batch, tokenizer):\n",
    "    system_prompt = \"\"\"\"You are a legal assistant specializing in U.S. immigration law. Think through each question and provide an answer. Don't make things up, if you're unable to answer a question advise the user that you're unable to answer as it is outside of your scope.\"\"\"\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # Access the inputs from the batch\n",
    "    questions = batch[\"question\"]\n",
    "    answers = batch[\"answer\"]\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        row_json = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": questions[i]},\n",
    "            {\"role\": \"assistant\", \"content\": answers[i]}\n",
    "        ]\n",
    "\n",
    "        # Apply chat template and append the result to the list\n",
    "        tokenizer.chat_template = \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "        text = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "        samples.append(text)\n",
    "\n",
    "    # Return a dictionary with lists as expected for batched processing\n",
    "    return {\n",
    "        \"instruction\": questions,\n",
    "        \"response\": answers,\n",
    "        \"text\": samples  # The processed chat template text for each row\n",
    "    }\n",
    "\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    trust_remote_code=True,\n",
    "    token=\"\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: format_chat_template(x, tokenizer), num_proc=8, batched=True, batch_size=10)\n",
    "print(train_dataset[0])\n",
    "\n",
    "validation_dataset = validation_dataset.map(lambda x: format_chat_template(x, tokenizer), num_proc=8, batched=True, batch_size=10)\n",
    "print(validation_dataset[0])\n",
    "\n",
    "test_dataset = test_dataset.map(lambda x: format_chat_template(x, tokenizer), num_proc=8, batched=True, batch_size=10)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f232d5-b254-4ce2-86d6-1c8606247cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(train_dataset)}\")\n",
    "\n",
    "local_train_file = \"train_dataset.jsonl\"\n",
    "train_dataset.to_json(local_train_file, lines=True)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3_prefix = \"processed/mistral\"\n",
    "s3_key = f\"{s3_prefix}/train_dataset.jsonl\"\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_key}\"\n",
    "\n",
    "s3.upload_file(local_train_file, sess.default_bucket(), s3_key)\n",
    "print(f\"Uploaded to s3://{sess.default_bucket()}/{s3_key}\")\n",
    "\n",
    "\n",
    "local_validation_file = \"validation_dataset.jsonl\"\n",
    "validation_dataset.to_json(local_validation_file, lines=True)\n",
    "\n",
    "s3_prefix = \"processed/mistral\"\n",
    "s3_key = f\"{s3_prefix}/validation_dataset.jsonl\"\n",
    "validation_input_path = f\"s3://{sess.default_bucket()}/{s3_key}\"\n",
    "\n",
    "s3.upload_file(local_validation_file, sess.default_bucket(), s3_key)\n",
    "print(f\"Uploaded to s3://{sess.default_bucket()}/{s3_key}\")\n",
    "\n",
    "\n",
    "local_test_file = \"test_dataset.jsonl\"\n",
    "test_dataset.to_json(local_test_file, lines=True)\n",
    "\n",
    "s3_prefix = \"processed/mistral\"\n",
    "s3_key = f\"{s3_prefix}/test_dataset.jsonl\"\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_key}\"\n",
    "\n",
    "s3.upload_file(local_test_file, sess.default_bucket(), s3_key)\n",
    "print(f\"Uploaded to s3://{sess.default_bucket()}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2430a051-228e-4b96-b8cc-1f1b29639431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "job_name = f'huggingface-qlora-{hyperparameters[\"model_id\"].replace(\"/\",\"-\").replace(\".\",\"-\")}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': base_model,                             # pre-trained model\n",
    "  # 'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'num_train_epochs': 2,                            # number of training epochs\n",
    "  'per_device_train_batch_size': 3,                 # batch size for training\n",
    "  'gradient_accumulation_steps': 2,                 # Number of updates steps to accumulate\n",
    "  'gradient_checkpointing': True,                   # save memory but slower backward pass\n",
    "  'bf16': True,                                     # use bfloat16 precision\n",
    "  'tf32': True,                                     # use tf32 precision\n",
    "  'learning_rate': 2e-4,                            # learning rate\n",
    "  'max_grad_norm': 0.3,                             # Maximum norm (for gradient clipping)\n",
    "  'warmup_ratio': 0.03,                             # warmup ratio\n",
    "  \"lr_scheduler_type\": \"constant\",                   # learning rate scheduler\n",
    "  'save_strategy': \"steps\",                         # save strategy for checkpoints\n",
    "  \"evaluation_strategy\": \"steps\",\n",
    "  \"eval_steps\": 50,\n",
    "  \"load_best_model_at_end\": True,\n",
    "  \"metric_for_best_model\": \"loss\",\n",
    "  \"greater_is_better\": False,\n",
    "  \"logging_steps\": 10,                              # log every x steps\n",
    "  \"merge_adapters\": True,                           # wether to merge LoRA into the model (needs more memory)\n",
    "  \"output_dir\": f\"s3://{sess.default_bucket()}/checkpoints/{job_name}\",                         # output directory, where to save assets during training\n",
    "  \"save_total_limit\": 2,                                                  # could be used for checkpointing. The final trained\n",
    "                                                    # model will always be saved to s3 at the end of training\n",
    "}\n",
    "\n",
    "if HfFolder.get_token() is not None:\n",
    "    hyperparameters['hf_token'] = HfFolder.get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "433de7db-663b-4948-81aa-8ce9317d5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_qlora.py',    # train script\n",
    "    source_dir           = './scripts',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True         # not compress output to save training time and cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61fe1e03-3860-429e-be95-92d03ececfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datasets import load_dataset\n",
      "    train_dataset = load_dataset(\"json\", data_files=\"/opt/ml/input/data/training/train_dataset.jsonl\")[\"train\"]\n",
      "    validation_dataset = load_dataset(\"json\", data_files=\"/opt/ml/input/data/validation/validation_dataset.jsonl\")[\"train\"]\n",
      "        load_in_4bit=True,\n",
      "File size: 15148.45 KB\n"
     ]
    }
   ],
   "source": [
    "!cat ./scripts/run_qlora.py | grep load_\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.head_object(Bucket=sess.default_bucket(), Key=\"processed/mistral/train_dataset.jsonl\")\n",
    "print(f\"File size: {obj['ContentLength'] / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5e094-6a4b-47a7-8588-551df2b8441e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-13 22:34:50 Starting - Starting the training job\n",
      "2025-06-13 22:34:50 Pending - Training job waiting for capacity.....\u001b[34m88%|████████▊ | 98/112 [11:37<01:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 99/112 [11:44<01:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 100/112 [11:51<01:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 101/112 [11:59<01:19,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 102/112 [12:06<01:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 103/112 [12:13<01:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 104/112 [12:20<00:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 105/112 [12:27<00:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 106/112 [12:35<00:43,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 107/112 [12:42<00:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 108/112 [12:49<00:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 109/112 [12:56<00:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 110/112 [13:03<00:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 111/112 [13:10<00:07,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.04931110143661499, 'eval_runtime': 800.2156, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.14, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 50/2372 [32:37<14:55:03, 23.13s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 51/2372 [33:00<169:43:11, 263.25s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 52/2372 [33:23<123:13:25, 191.21s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 53/2372 [33:46<90:41:23, 140.79s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 54/2372 [34:09<67:55:22, 105.49s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 55/2372 [34:32<51:59:27, 80.78s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 56/2372 [34:56<40:50:28, 63.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 57/2372 [35:19<33:02:18, 51.38s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 58/2372 [35:42<27:34:33, 42.90s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 59/2372 [36:05<23:45:07, 36.97s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2372 [36:28<21:04:29, 32.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0491, 'grad_norm': 0.041015625, 'learning_rate': 0.0002, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m3%|▎         | 60/2372 [36:28<21:04:29, 32.82s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 61/2372 [36:51<19:13:21, 29.94s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 62/2372 [37:14<17:54:08, 27.90s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 63/2372 [37:38<16:58:37, 26.47s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 64/2372 [38:01<16:19:38, 25.47s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 65/2372 [38:24<15:52:13, 24.77s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 66/2372 [38:47<15:32:54, 24.27s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 67/2372 [39:10<15:19:18, 23.93s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 68/2372 [39:33<15:09:38, 23.69s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 69/2372 [39:56<15:02:47, 23.52s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2372 [40:19<14:57:53, 23.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0429, 'grad_norm': 0.037353515625, 'learning_rate': 0.0002, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m3%|▎         | 70/2372 [40:20<14:57:53, 23.40s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 71/2372 [40:43<14:55:14, 23.34s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 72/2372 [41:06<14:52:24, 23.28s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 73/2372 [41:29<14:50:16, 23.23s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 74/2372 [41:52<14:48:39, 23.20s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 75/2372 [42:15<14:47:25, 23.18s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 76/2372 [42:38<14:46:26, 23.17s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 77/2372 [43:01<14:45:39, 23.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 78/2372 [43:25<14:44:59, 23.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 79/2372 [43:48<14:44:23, 23.14s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2372 [44:11<14:43:51, 23.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0445, 'grad_norm': 0.035400390625, 'learning_rate': 0.0002, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m3%|▎         | 80/2372 [44:11<14:43:51, 23.14s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 81/2372 [44:34<14:44:11, 23.16s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 82/2372 [44:57<14:43:28, 23.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 83/2372 [45:20<14:42:50, 23.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 84/2372 [45:43<14:42:19, 23.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 85/2372 [46:07<14:41:48, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 86/2372 [46:30<14:41:24, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 87/2372 [46:53<14:40:59, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 88/2372 [47:16<14:40:32, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 89/2372 [47:39<14:40:07, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2372 [48:02<14:39:41, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0409, 'grad_norm': 0.0306396484375, 'learning_rate': 0.0002, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m4%|▍         | 90/2372 [48:02<14:39:41, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 91/2372 [48:25<14:40:07, 23.15s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 92/2372 [48:49<14:39:30, 23.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 93/2372 [49:12<14:38:55, 23.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 94/2372 [49:35<14:38:25, 23.14s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 95/2372 [49:58<14:37:57, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 96/2372 [50:21<14:37:31, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 97/2372 [50:44<14:37:06, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 98/2372 [51:07<14:36:38, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 99/2372 [51:30<14:36:12, 23.13s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2372 [51:54<14:35:48, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0391, 'grad_norm': 0.027587890625, 'learning_rate': 0.0002, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m4%|▍         | 100/2372 [51:54<14:35:48, 23.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/112 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/112 [00:07<06:35,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/112 [00:14<09:15,  5.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/112 [00:21<10:34,  5.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/112 [00:28<11:17,  6.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/112 [00:35<11:41,  6.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 7/112 [00:43<11:54,  6.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/112 [00:50<12:00,  6.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/112 [00:57<12:01,  7.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/112 [01:04<12:00,  7.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/112 [01:11<11:57,  7.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 12/112 [01:19<11:52,  7.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 13/112 [01:26<11:47,  7.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 14/112 [01:33<11:41,  7.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/112 [01:40<11:35,  7.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/112 [01:47<11:28,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 17/112 [01:55<11:22,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/112 [02:02<11:15,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/112 [02:09<11:08,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/112 [02:16<11:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 21/112 [02:23<10:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 22/112 [02:31<10:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 23/112 [02:38<10:39,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 24/112 [02:45<10:32,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/112 [02:52<10:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/112 [02:59<10:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 27/112 [03:06<10:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 28/112 [03:14<10:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 29/112 [03:21<09:56,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 30/112 [03:28<09:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 31/112 [03:35<09:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 32/112 [03:42<09:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 33/112 [03:50<09:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 34/112 [03:57<09:20,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 35/112 [04:04<09:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 36/112 [04:11<09:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 37/112 [04:18<08:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 38/112 [04:26<08:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 39/112 [04:33<08:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 40/112 [04:40<08:37,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 41/112 [04:47<08:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 42/112 [04:54<08:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 43/112 [05:02<08:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 44/112 [05:09<08:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 45/112 [05:16<08:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 46/112 [05:23<07:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 47/112 [05:30<07:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 48/112 [05:38<07:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 49/112 [05:45<07:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 50/112 [05:52<07:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 51/112 [05:59<07:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 52/112 [06:06<07:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 53/112 [06:13<07:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 54/112 [06:21<06:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 55/112 [06:28<06:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 56/112 [06:35<06:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 57/112 [06:42<06:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 58/112 [06:49<06:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 59/112 [06:57<06:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 60/112 [07:04<06:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 61/112 [07:11<06:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 62/112 [07:18<05:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 63/112 [07:25<05:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 64/112 [07:33<05:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 65/112 [07:40<05:38,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 66/112 [07:47<05:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 67/112 [07:54<05:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 68/112 [08:01<05:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 69/112 [08:09<05:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 70/112 [08:16<05:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 71/112 [08:23<04:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 72/112 [08:30<04:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 73/112 [08:37<04:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 74/112 [08:44<04:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 75/112 [08:52<04:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 76/112 [08:59<04:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 77/112 [09:06<04:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 78/112 [09:13<04:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 79/112 [09:20<03:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 80/112 [09:28<03:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 81/112 [09:35<03:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 82/112 [09:42<03:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 83/112 [09:49<03:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 84/112 [09:56<03:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 85/112 [10:04<03:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 86/112 [10:11<03:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 87/112 [10:18<02:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 88/112 [10:25<02:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 89/112 [10:32<02:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 90/112 [10:40<02:38,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 91/112 [10:47<02:31,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 92/112 [10:54<02:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 93/112 [11:01<02:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 94/112 [11:08<02:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 95/112 [11:16<02:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 96/112 [11:23<01:55,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 97/112 [11:30<01:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 98/112 [11:37<01:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 99/112 [11:44<01:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 100/112 [11:51<01:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 101/112 [11:59<01:19,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 102/112 [12:06<01:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 103/112 [12:13<01:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 104/112 [12:20<00:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 105/112 [12:27<00:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 106/112 [12:35<00:43,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 107/112 [12:42<00:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 108/112 [12:49<00:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 109/112 [12:56<00:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 110/112 [13:03<00:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 111/112 [13:11<00:07,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.04000714048743248, 'eval_runtime': 800.2896, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.14, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A#015  4%|▍         | 100/2372 [1:05:14<14:35:48, 23.13s/it]\u001b[0m\n",
      "\u001b[34m#015                                                 #033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 101/2372 [1:05:37<166:04:37, 263.27s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 102/2372 [1:06:00<120:34:39, 191.22s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 103/2372 [1:06:23<88:44:24, 140.80s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 104/2372 [1:06:47<66:27:44, 105.50s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 105/2372 [1:07:10<50:52:21, 80.79s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 106/2372 [1:07:33<39:57:47, 63.49s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 107/2372 [1:07:56<32:19:39, 51.38s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 108/2372 [1:08:19<26:59:00, 42.91s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 109/2372 [1:08:42<23:14:29, 36.97s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2372 [1:09:05<20:37:17, 32.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0409, 'grad_norm': 0.0255126953125, 'learning_rate': 0.0002, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m5%|▍         | 110/2372 [1:09:05<20:37:17, 32.82s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 111/2372 [1:09:29<18:48:14, 29.94s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 112/2372 [1:09:52<17:30:47, 27.90s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 113/2372 [1:10:15<16:36:27, 26.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 114/2372 [1:10:38<15:58:22, 25.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 115/2372 [1:11:01<15:31:32, 24.76s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 116/2372 [1:11:24<15:12:39, 24.27s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 117/2372 [1:11:47<14:59:21, 23.93s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 118/2372 [1:12:10<14:49:54, 23.69s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 119/2372 [1:12:34<14:43:13, 23.52s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2372 [1:12:57<14:38:25, 23.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0417, 'grad_norm': 0.02587890625, 'learning_rate': 0.0002, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m5%|▌         | 120/2372 [1:12:57<14:38:25, 23.40s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 121/2372 [1:13:20<14:36:01, 23.35s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 122/2372 [1:13:43<14:33:09, 23.28s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 123/2372 [1:14:06<14:31:00, 23.24s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 124/2372 [1:14:29<14:29:24, 23.20s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 125/2372 [1:14:52<14:28:09, 23.18s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 126/2372 [1:15:16<14:27:10, 23.17s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 127/2372 [1:15:39<14:26:20, 23.15s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 128/2372 [1:16:02<14:25:38, 23.15s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 129/2372 [1:16:25<14:25:05, 23.14s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2372 [1:16:48<14:24:33, 23.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0418, 'grad_norm': 0.030029296875, 'learning_rate': 0.0002, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m5%|▌         | 130/2372 [1:16:48<14:24:33, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 131/2372 [1:17:11<14:24:52, 23.16s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 132/2372 [1:17:34<14:24:09, 23.15s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 133/2372 [1:17:58<14:23:36, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 134/2372 [1:18:21<14:23:02, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 135/2372 [1:18:44<14:22:32, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 136/2372 [1:19:07<14:22:05, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 137/2372 [1:19:30<14:21:38, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 138/2372 [1:19:53<14:21:13, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 139/2372 [1:20:16<14:20:50, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2372 [1:20:39<14:20:27, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0384, 'grad_norm': 0.027099609375, 'learning_rate': 0.0002, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m6%|▌         | 140/2372 [1:20:40<14:20:27, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 141/2372 [1:21:03<14:20:55, 23.15s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 142/2372 [1:21:26<14:20:17, 23.15s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 143/2372 [1:21:49<14:19:41, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 144/2372 [1:22:12<14:19:11, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 145/2372 [1:22:35<14:18:42, 23.14s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 146/2372 [1:22:58<14:18:15, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 147/2372 [1:23:21<14:17:48, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 148/2372 [1:23:45<14:17:24, 23.13s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2372 [1:24:31<14:16:38, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.038, 'grad_norm': 0.0264892578125, 'learning_rate': 0.0002, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2372 [1:24:31<14:16:38, 23.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/112 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/112 [00:07<06:35,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/112 [00:14<09:15,  5.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/112 [00:21<10:34,  5.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/112 [00:28<11:17,  6.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/112 [00:35<11:41,  6.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 7/112 [00:43<11:54,  6.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/112 [00:50<12:00,  6.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/112 [00:57<12:01,  7.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/112 [01:04<12:00,  7.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/112 [01:11<11:57,  7.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 12/112 [01:19<11:52,  7.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 13/112 [01:26<11:47,  7.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 14/112 [01:33<11:41,  7.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/112 [01:40<11:35,  7.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/112 [01:47<11:28,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 17/112 [01:55<11:22,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/112 [02:02<11:15,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/112 [02:09<11:08,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/112 [02:16<11:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 21/112 [02:23<10:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 22/112 [02:31<10:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 23/112 [02:38<10:39,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 24/112 [02:45<10:32,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/112 [02:52<10:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/112 [02:59<10:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 27/112 [03:06<10:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 28/112 [03:14<10:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 29/112 [03:21<09:56,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 30/112 [03:28<09:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 31/112 [03:35<09:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 32/112 [03:42<09:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 33/112 [03:50<09:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 34/112 [03:57<09:20,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 35/112 [04:04<09:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 36/112 [04:11<09:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 37/112 [04:18<08:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 38/112 [04:26<08:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 39/112 [04:33<08:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 40/112 [04:40<08:37,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 41/112 [04:47<08:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 42/112 [04:54<08:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 43/112 [05:02<08:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 44/112 [05:09<08:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 45/112 [05:16<08:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 46/112 [05:23<07:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 47/112 [05:30<07:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 48/112 [05:38<07:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 49/112 [05:45<07:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 50/112 [05:52<07:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 51/112 [05:59<07:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 52/112 [06:06<07:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 53/112 [06:13<07:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 54/112 [06:21<06:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 55/112 [06:28<06:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 56/112 [06:35<06:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 57/112 [06:42<06:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 58/112 [06:49<06:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 59/112 [06:57<06:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 60/112 [07:04<06:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 61/112 [07:11<06:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 62/112 [07:18<05:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 63/112 [07:25<05:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 64/112 [07:33<05:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 65/112 [07:40<05:38,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 66/112 [07:47<05:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 67/112 [07:54<05:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 68/112 [08:01<05:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 69/112 [08:09<05:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 70/112 [08:16<05:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 71/112 [08:23<04:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 72/112 [08:30<04:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 73/112 [08:37<04:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 74/112 [08:45<04:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 75/112 [08:52<04:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 76/112 [08:59<04:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 77/112 [09:06<04:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 78/112 [09:13<04:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 79/112 [09:20<03:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 80/112 [09:28<03:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 81/112 [09:35<03:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 82/112 [09:42<03:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 83/112 [09:49<03:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 84/112 [09:56<03:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 85/112 [10:04<03:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 86/112 [10:11<03:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 87/112 [10:18<02:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 88/112 [10:25<02:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 89/112 [10:32<02:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 90/112 [10:40<02:38,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 91/112 [10:47<02:31,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 92/112 [10:54<02:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 93/112 [11:01<02:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 94/112 [11:08<02:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 95/112 [11:16<02:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 96/112 [11:23<01:55,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 97/112 [11:30<01:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 98/112 [11:37<01:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 99/112 [11:44<01:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 100/112 [11:51<01:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 101/112 [11:59<01:19,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 102/112 [12:06<01:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 103/112 [12:13<01:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 104/112 [12:20<00:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 105/112 [12:27<00:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 106/112 [12:35<00:43,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 107/112 [12:42<00:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 108/112 [12:49<00:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 109/112 [12:56<00:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 110/112 [13:03<00:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 111/112 [13:11<00:07,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.03967493027448654, 'eval_runtime': 800.3281, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.14, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:15<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 150/2372 [1:37:51<14:16:38, 23.13s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 151/2372 [1:38:14<162:25:36, 263.28s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 152/2372 [1:38:38<117:55:34, 191.23s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 153/2372 [1:39:01<86:47:16, 140.80s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 154/2372 [1:39:24<64:59:57, 105.50s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 155/2372 [1:39:47<49:45:09, 80.79s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 156/2372 [1:40:10<39:04:59, 63.49s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 157/2372 [1:40:33<31:36:55, 51.38s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 158/2372 [1:40:56<26:23:18, 42.91s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 159/2372 [1:41:19<22:43:43, 36.97s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2372 [1:41:43<20:10:00, 32.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0378, 'grad_norm': 0.024169921875, 'learning_rate': 0.0002, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m7%|▋         | 160/2372 [1:41:43<20:10:00, 32.82s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 161/2372 [1:42:06<18:23:08, 29.94s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 162/2372 [1:42:29<17:07:26, 27.89s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 163/2372 [1:42:52<16:14:22, 26.47s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 164/2372 [1:43:15<15:37:06, 25.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 165/2372 [1:43:38<15:10:51, 24.76s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 166/2372 [1:44:01<14:52:27, 24.27s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 167/2372 [1:44:25<14:39:24, 23.93s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 168/2372 [1:44:48<14:30:11, 23.69s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 169/2372 [1:45:11<14:23:37, 23.52s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2372 [1:45:34<14:18:53, 23.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0413, 'grad_norm': 0.025390625, 'learning_rate': 0.0002, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m7%|▋         | 170/2372 [1:45:34<14:18:53, 23.40s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 171/2372 [1:45:57<14:16:14, 23.34s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 172/2372 [1:46:20<14:13:29, 23.28s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 173/2372 [1:46:43<14:11:28, 23.23s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 174/2372 [1:47:07<14:09:57, 23.20s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 175/2372 [1:47:30<14:08:46, 23.18s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 176/2372 [1:47:53<14:07:49, 23.16s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 177/2372 [1:48:16<14:07:03, 23.15s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 178/2372 [1:48:39<14:06:23, 23.15s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 179/2372 [1:49:02<14:05:49, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2372 [1:49:25<14:05:18, 23.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0408, 'grad_norm': 0.0257568359375, 'learning_rate': 0.0002, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m8%|▊         | 180/2372 [1:49:25<14:05:18, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 181/2372 [1:49:49<14:05:55, 23.17s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 182/2372 [1:50:12<14:05:10, 23.16s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 183/2372 [1:50:35<14:04:28, 23.15s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 184/2372 [1:50:58<14:03:54, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 185/2372 [1:51:21<14:03:23, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 186/2372 [1:51:44<14:02:51, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 187/2372 [1:52:07<14:02:28, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 188/2372 [1:52:30<14:02:03, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 189/2372 [1:52:54<14:01:37, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2372 [1:53:17<14:01:12, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0397, 'grad_norm': 0.0234375, 'learning_rate': 0.0002, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m8%|▊         | 190/2372 [1:53:17<14:01:12, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 191/2372 [1:53:40<14:01:53, 23.16s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 192/2372 [1:54:03<14:01:09, 23.15s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 193/2372 [1:54:26<14:00:31, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 194/2372 [1:54:49<13:59:57, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 195/2372 [1:55:12<13:59:29, 23.14s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 196/2372 [1:55:36<13:59:00, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 197/2372 [1:55:59<13:58:31, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 198/2372 [1:56:22<13:58:08, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 199/2372 [1:56:45<13:57:45, 23.13s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2372 [1:57:08<13:57:21, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0401, 'grad_norm': 0.02783203125, 'learning_rate': 0.0002, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m8%|▊         | 200/2372 [1:57:08<13:57:21, 23.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/112 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/112 [00:07<06:35,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/112 [00:14<09:15,  5.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/112 [00:21<10:34,  5.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/112 [00:28<11:17,  6.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/112 [00:35<11:41,  6.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 7/112 [00:43<11:54,  6.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/112 [00:50<12:00,  6.93s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/112 [00:57<12:01,  7.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/112 [01:04<12:00,  7.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/112 [01:11<11:57,  7.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 12/112 [01:19<11:53,  7.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 13/112 [01:26<11:47,  7.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 14/112 [01:33<11:41,  7.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/112 [01:40<11:35,  7.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/112 [01:47<11:29,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 17/112 [01:55<11:22,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/112 [02:02<11:15,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/112 [02:09<11:08,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/112 [02:16<11:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 21/112 [02:23<10:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 22/112 [02:31<10:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 23/112 [02:38<10:39,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 24/112 [02:45<10:32,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/112 [02:52<10:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/112 [02:59<10:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 27/112 [03:06<10:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 28/112 [03:14<10:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 29/112 [03:21<09:56,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 30/112 [03:28<09:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 336/2372 [3:29:36<13:05:03, 23.14s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 337/2372 [3:29:59<13:04:39, 23.13s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 338/2372 [3:30:22<13:04:10, 23.13s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 339/2372 [3:30:45<13:03:45, 23.13s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2372 [3:31:08<13:03:18, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0373, 'grad_norm': 0.024169921875, 'learning_rate': 0.0002, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 340/2372 [3:31:09<13:03:18, 23.13s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 341/2372 [3:31:32<13:03:40, 23.15s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 342/2372 [3:31:55<13:03:02, 23.14s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 343/2372 [3:32:18<13:02:28, 23.14s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 344/2372 [3:32:41<13:02:02, 23.14s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 345/2372 [3:33:04<13:01:32, 23.13s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 346/2372 [3:33:27<13:01:05, 23.13s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 347/2372 [3:33:50<13:00:41, 23.13s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 348/2372 [3:34:14<13:00:18, 23.13s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 349/2372 [3:34:37<12:59:55, 23.13s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2372 [3:35:00<12:59:30, 23.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0374, 'grad_norm': 0.026611328125, 'learning_rate': 0.0002, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2372 [3:35:00<12:59:30, 23.13s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/112 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/112 [00:07<06:35,  3.60s/it]#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/112 [00:14<09:15,  5.09s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▎         | 4/112 [00:21<10:34,  5.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/112 [00:28<11:17,  6.33s/it]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/112 [00:35<11:41,  6.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m6%|▋         | 7/112 [00:43<11:54,  6.80s/it]#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/112 [00:50<12:00,  6.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/112 [00:57<12:01,  7.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 10/112 [01:04<12:00,  7.06s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/112 [01:11<11:57,  7.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 12/112 [01:19<11:52,  7.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 13/112 [01:26<11:47,  7.15s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 14/112 [01:33<11:41,  7.16s/it]#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/112 [01:40<11:35,  7.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/112 [01:47<11:28,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 17/112 [01:55<11:22,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/112 [02:02<11:15,  7.18s/it]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/112 [02:09<11:08,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 20/112 [02:16<11:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m19%|█▉        | 21/112 [02:23<10:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|█▉        | 22/112 [02:31<10:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██        | 23/112 [02:38<10:39,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 24/112 [02:45<10:32,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/112 [02:52<10:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/112 [02:59<10:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 27/112 [03:06<10:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 28/112 [03:14<10:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m26%|██▌       | 29/112 [03:21<09:56,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 30/112 [03:28<09:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 31/112 [03:35<09:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 32/112 [03:42<09:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 33/112 [03:50<09:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 34/112 [03:57<09:20,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m31%|███▏      | 35/112 [04:04<09:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 36/112 [04:11<09:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 37/112 [04:18<08:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m34%|███▍      | 38/112 [04:26<08:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 39/112 [04:33<08:44,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 40/112 [04:40<08:37,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 41/112 [04:47<08:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 42/112 [04:54<08:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 43/112 [05:02<08:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 44/112 [05:09<08:08,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 45/112 [05:16<08:01,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m41%|████      | 46/112 [05:23<07:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▏     | 47/112 [05:30<07:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 48/112 [05:37<07:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 49/112 [05:45<07:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▍     | 50/112 [05:52<07:25,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▌     | 51/112 [05:59<07:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m46%|████▋     | 52/112 [06:06<07:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 53/112 [06:13<07:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 54/112 [06:21<06:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 55/112 [06:28<06:49,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 56/112 [06:35<06:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 57/112 [06:42<06:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 58/112 [06:49<06:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 59/112 [06:57<06:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 60/112 [07:04<06:13,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 61/112 [07:11<06:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 62/112 [07:18<05:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 63/112 [07:25<05:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 64/112 [07:33<05:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 65/112 [07:40<05:37,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 66/112 [07:47<05:30,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 67/112 [07:54<05:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 68/112 [08:01<05:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 69/112 [08:08<05:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 70/112 [08:16<05:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 71/112 [08:23<04:54,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 72/112 [08:30<04:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 73/112 [08:37<04:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 74/112 [08:44<04:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 75/112 [08:52<04:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 76/112 [08:59<04:18,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 77/112 [09:06<04:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 78/112 [09:13<04:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 79/112 [09:20<03:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 80/112 [09:28<03:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 81/112 [09:35<03:42,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 82/112 [09:42<03:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 83/112 [09:49<03:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 84/112 [09:56<03:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 85/112 [10:04<03:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 86/112 [10:11<03:06,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 87/112 [10:18<02:59,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 88/112 [10:25<02:52,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 89/112 [10:32<02:45,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 90/112 [10:40<02:38,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 91/112 [10:47<02:31,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 92/112 [10:54<02:23,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 93/112 [11:01<02:16,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 94/112 [11:08<02:09,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 95/112 [11:15<02:02,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 96/112 [11:23<01:55,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 97/112 [11:30<01:47,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 98/112 [11:37<01:40,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 99/112 [11:44<01:33,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 100/112 [11:51<01:26,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 101/112 [11:59<01:19,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 102/112 [12:06<01:11,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 103/112 [12:13<01:04,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 104/112 [12:20<00:57,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 105/112 [12:27<00:50,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 106/112 [12:35<00:43,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 107/112 [12:42<00:35,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 108/112 [12:49<00:28,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 109/112 [12:56<00:21,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 110/112 [13:03<00:14,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 111/112 [13:11<00:07,  7.19s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.03872128203511238, 'eval_runtime': 800.2457, 'eval_samples_per_second': 1.112, 'eval_steps_per_second': 0.14, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m100%|██████████| 112/112 [13:14<00:00,  6.03s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 350/2372 [3:48:20<12:59:30, 23.13s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 351/2372 [3:48:43<147:47:04, 263.25s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 352/2372 [3:49:06<107:17:28, 191.21s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 353/2372 [3:49:30<78:57:30, 140.79s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 354/2372 [3:49:53<59:07:58, 105.49s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 355/2372 [3:50:16<45:15:36, 80.78s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 356/2372 [3:50:39<35:33:08, 63.49s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 357/2372 [3:51:02<28:45:29, 51.38s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 358/2372 [3:51:25<24:00:09, 42.90s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 359/2372 [3:51:48<20:40:26, 36.97s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2372 [3:52:12<18:20:33, 32.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0391, 'grad_norm': 0.0244140625, 'learning_rate': 0.0002, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 360/2372 [3:52:12<18:20:33, 32.82s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 361/2372 [3:52:35<16:43:20, 29.94s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 362/2372 [3:52:58<15:34:26, 27.89s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 363/2372 [3:53:21<14:46:08, 26.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 364/2372 [3:53:44<14:12:09, 25.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 365/2372 [3:54:07<13:48:19, 24.76s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 366/2372 [3:54:30<13:31:29, 24.27s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 367/2372 [3:54:54<13:19:36, 23.93s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 368/2372 [3:55:17<13:11:11, 23.69s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 369/2372 [3:55:40<13:05:15, 23.52s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 370/2372 [3:56:03<13:00:57, 23.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0401, 'grad_norm': 0.0250244140625, 'learning_rate': 0.0002, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 370/2372 [3:56:03<13:00:57, 23.41s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 371/2372 [3:56:26<12:58:40, 23.35s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 372/2372 [3:56:49<12:56:05, 23.28s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 373/2372 [3:57:12<12:54:10, 23.24s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 374/2372 [3:57:36<12:52:41, 23.20s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 375/2372 [3:57:59<12:51:32, 23.18s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'training': training_input_path,\n",
    "    'validation': validation_input_path\n",
    "}\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbc5f588-e222-45e2-8882-2245da1d90c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-329599621791/huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126/output/model/'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"].replace(\"s3://\", \"https://s3.console.aws.amazon.com/s3/buckets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44c86df4-ba49-4b70-8de4-e23898dce198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.1.0\",\n",
    "  session=sess,\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16dda499-08fd-430f-a066-14122508f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f /home/ec2-user/SageMaker/model/tokenizer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be54e1fe-99aa-4260-ba33-9d3696f89bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine_tune_mistral7B  lost+found  model\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29edbf43-e15c-4f0d-a315-b5ff4b689d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs         16G     0   16G   0% /dev\n",
      "tmpfs            16G     0   16G   0% /dev/shm\n",
      "tmpfs            16G  688K   16G   1% /run\n",
      "tmpfs            16G     0   16G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  135G   89G   47G  66% /\n",
      "tmpfs           3.1G     0  3.1G   0% /run/user/0\n",
      "/dev/nvme2n1     30G   14G   15G  49% /home/ec2-user/SageMaker\n",
      "tmpfs           3.1G     0  3.1G   0% /run/user/1002\n",
      "tmpfs           3.1G     0  3.1G   0% /run/user/1001\n",
      "tmpfs           3.1G     0  3.1G   0% /run/user/1000\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa83995-8e92-40c3-a1e2-da0cffc71980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to s3://sagemaker-us-east-1-329599621791/huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126/output/compressed_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# print(huggingface_estimator.model_data[\"S3DataSource\"])\n",
    "\n",
    "prefix = \"huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126/output/model/\"  # with trailing slash\n",
    "model_dir = \"/home/ec2-user/SageMaker/model\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(sess.default_bucket())\n",
    "# for obj in bucket.objects.filter(Prefix=prefix):\n",
    "#     target = os.path.join(model_dir, os.path.relpath(obj.key, prefix))\n",
    "#     os.makedirs(os.path.dirname(target), exist_ok=True)\n",
    "#     bucket.download_file(obj.key, target)\n",
    "\n",
    "tarball_path = \"model.tar.gz\"\n",
    "\n",
    "# with tarfile.open(tarball_path, \"w:gz\") as tar:\n",
    "#     tar.add(model_dir, arcname=\".\")\n",
    "\n",
    "s3_prefix = \"huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126/output/compressed_model\"\n",
    "s3_key = f\"{s3_prefix}/model.tar.gz\"\n",
    "\n",
    "s3.meta.client.upload_file(tarball_path, sess.default_bucket(), s3_key)\n",
    "print(f\"Uploaded to s3://{sess.default_bucket()}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d7365-e02c-4b46-b61b-8e444e57e6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID': 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    'SM_NUM_GPUS': json.dumps(1),\n",
    "    'HUGGING_FACE_HUB_TOKEN':'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'\n",
    "}\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    image_uri=llm_image,\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    model_data=f\"s3://{sess.default_bucket()}/huggingface-qlora-mistralai-Mistral-7B--2025-06-13-22-34-50-126/output/compressed_model/model.tar.gz\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "predictor = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92aa41-97a5-465a-9032-ef19b3f6ed02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
